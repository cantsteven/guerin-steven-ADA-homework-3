---
title: "guerin-steven-ADA-homework-3"
author: "Steven"
date: "4/23/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Challenge 1

```{r challenge 1}
library(ggplot2)
library(tidyverse)
library(dplyr)
library(broom)
library(infer)


f <- "https://raw.githubusercontent.com/difiore/ADA-datasets/master/KamilarAndCooperData.csv"
d <- read_csv(f, col_names = TRUE)


#Challenge 1a-----------------------------------------------------
#Untransformed Graph
originalmodel<-lm(MaxLongevity_m ~ Brain_Size_Species_Mean,data=d)
originalmodel

originalmodelgraph<-ggplot(data=d,aes(x=Brain_Size_Species_Mean,y=MaxLongevity_m))+
  geom_point()+
  geom_smooth(method="lm")+
   annotate(geom = "text", x = 300, y = 300, label = "y=1.2x+249.0", hjust = "left")
  
originalmodelgraph

#Transformed Graph 
logmodel<-lm(log(MaxLongevity_m) ~ log(Brain_Size_Species_Mean),data=d)
logmodel

logmodelgraph<-ggplot(data=d,aes(x=log(Brain_Size_Species_Mean),y=log(MaxLongevity_m)))+
  geom_point()+
  geom_smooth(method="lm")+
  annotate(geom = "text", x = 4, y = 5, label = "y=0.2x+4.9", hjust = "left")

logmodelgraph

#Challenge 1b and c-----------------------------------------------------
#Original Model: reject the null, accept the alternative hypothesis 
b1 <- originalmodel$coefficients[2]
b1

confint<- predict(originalmodel, newdata=list(Brain_Size_Species_Mean=b1), interval="confidence", level=.90)
confint

#reject the null, accept the alternative hypothesis 
#Log Model: reject the null, accept the alternative hypothesis 
b1 <- logmodel$coefficients[2]
b1

confint<- predict(logmodel, newdata=list(Brain_Size_Species_Mean=b1), interval="confidence", level=.90)
confint

#original prediction, confidence intervals, and graph 
alpha<-0.1
b <- seq(from = 0, to = 500, by = 1)
sd <- glance(originalmodel) %>% pull(sigma)
df <- augment(originalmodel, newdata = data.frame(Brain_Size_Species_Mean = b)) %>%
 mutate(
    c.lwr = .fitted - qt(1 - alpha / 2, nrow(d) - 2) * .se.fit,
    c.upr = .fitted + qt(1 - alpha / 2, nrow(d) - 2) * .se.fit
  ) %>%
  # add PI
  mutate(
    se.prediction = sqrt(sd^2 + .se.fit^2),
    p.lwr = .fitted - qt(1 - alpha / 2, nrow(d) - 2) * se.prediction,
    p.upr = .fitted + qt(1 - alpha / 2, nrow(d) - 2) * se.prediction
  )

originalmodelgraph<-ggplot(data=d,aes(x=Brain_Size_Species_Mean,y=MaxLongevity_m))+
  geom_point()+
geom_line(
  data = df, aes(x=Brain_Size_Species_Mean, y = c.lwr, color = "blue"))+
geom_line(
  data = df, aes(x=Brain_Size_Species_Mean, y = c.upr,color = "blue"))+
geom_line(
  data = df, aes(x=Brain_Size_Species_Mean, y = p.lwr,color = "red"))+
geom_line(
  data = df, aes(x=Brain_Size_Species_Mean, y = p.upr,color = "red"))+
scale_color_discrete(name = "Lines", labels = c("90% Confidence Intervals", "90% Prediction Intervals"))

originalmodelgraph

#log prediction, confidence intervals, and graph 
alpha<-0.1
b <- seq(from = 0, to = 500, by = 1)
sd <- glance(logmodel) %>% pull(sigma)
df <- augment(logmodel, newdata = data.frame(Brain_Size_Species_Mean = b)) %>%
 mutate(
    c.lwr = .fitted - qt(1 - alpha / 2, nrow(d) - 2) * .se.fit,
    c.upr = .fitted + qt(1 - alpha / 2, nrow(d) - 2) * .se.fit
  ) %>%
  # add PI
  mutate(
    se.prediction = sqrt(sd^2 + .se.fit^2),
    p.lwr = .fitted - qt(1 - alpha / 2, nrow(d) - 2) * se.prediction,
    p.upr = .fitted + qt(1 - alpha / 2, nrow(d) - 2) * se.prediction
  )

logmodelgraph<-ggplot(data=d,aes(x=log(Brain_Size_Species_Mean),y=log(MaxLongevity_m)))+
  geom_point()+
geom_line(
  data = df, aes(x=log(Brain_Size_Species_Mean), y = c.lwr, color = "blue"))+
geom_line(
  data = df, aes(x=log(Brain_Size_Species_Mean), y = c.upr,color = "blue"))+
geom_line(
  data = df, aes(x=log(Brain_Size_Species_Mean), y = p.lwr,color = "red"))+
geom_line(
  data = df, aes(x=log(Brain_Size_Species_Mean), y = p.upr,color = "red"))+
scale_color_discrete(name = "Lines", labels = c("90% Confidence Intervals", "90% Prediction Intervals"))

logmodelgraph

#Challenge 1d-----------------------------------------------------
#original model 
pint<- predict(originalmodel, newdata=list(Brain_Size_Species_Mean=750), interval="prediction", level=.90)
pint

#log model 
pint<- predict(logmodel, newdata=list(Brain_Size_Species_Mean=750), interval="prediction", level=.90)
pint

#This data point is way outside the range of values given in the dataset, and I'm assuming it's probably good to be skeptical of making estimates like this!

#Challenge 1e-----------------------------------------------------
#I think the log transformed model is better. The original model had too many data points at the lower end of the range of brain sizes, the log model spreads the data out more and makes it easier to see what the data looks like. 

```

## Challenge 2
```{r challenge2, echo=FALSE}

#Challenge 2a-----------------------------------------------------

rangemodel<- lm(log(HomeRange_km2) ~  log(Body_mass_female_mean), data=d)
b0 <- rangemodel$coefficients[1]
b0
b1 <- rangemodel$coefficients[2]
b1


#Challenge 2b-----------------------------------------------------
#adding the log values to the original data frame 
d <- mutate(d, LHR = log(HomeRange_km2))
d <- mutate(d, LBM = log(Body_mass_female_mean))

#bootstrapping 
boot.strap <- d %>% 
  specify(LHR ~ LBM) %>% 
  generate(reps = 1000, type = "bootstrap")
slope <- vector()
intercept <- vector()

for(i in 1:130) {
  x <- filter(boot.strap, replicate == i)
  y <- lm(LHR ~ LBM, data = x)
  slope[[i]] <- y$coefficients[2]
  intercept[[i]] <- y$coefficients[1]
}

boot.strap_lm <- tibble(
  slope = slope, 
  intercept = intercept)

hist(boot.strap_lm$slope,
  main = "Histogram of b1 values",
  xlab = "Slope")

hist(boot.strap_lm$intercept,
  main = "Histogram of b0 values",
  xlab = "Intercept")


#Challenge 2c and d-----------------------------------------------------

#for b1
alpha <- 0.05
confidence_level <- 1 - alpha
p_lower <- alpha / 2
p_upper <- 1 - (alpha / 2)
degrees_of_freedom <- nrow(boot.strap_lm) - 2
critical_value <- qt(p_upper, df = degrees_of_freedom)


#Creating confidence intervals 
permuted.slope.summary <- boot.strap_lm %>%
  # summarize the mean, t distribution based CI, and quantile-based CI
  summarize(
    estimate = mean(slope),
    std.error = sd(slope),
    lower = estimate - std.error * critical_value,
    upper = estimate + std.error * critical_value,
    perm.lower = quantile(slope, p_lower),
    perm.upper = quantile(slope, p_upper)
  )
permuted.slope.summary

#b0
alpha <- 0.05
confidence_level <- 1 - alpha
p_lower <- alpha / 2
p_upper <- 1 - (alpha / 2)
degrees_of_freedom <- nrow(boot.strap_lm) - 2
critical_value <- qt(p_upper, df = degrees_of_freedom)

#Creating confidence intervals 
permuted.intercept.summary <- boot.strap_lm %>%
  # summarize the mean, t distribution based CI, and quantile-based CI
  summarize(
    estimate = mean(intercept),
    std.error = sd(intercept),
    lower = estimate - std.error * critical_value,
    upper = estimate + std.error * critical_value,
    perm.lower = quantile(slope, p_lower),
    perm.upper = quantile(slope, p_upper)
  )
permuted.intercept.summary

```

##Challenge 3
```{r challenge3, echo=FALSE}
boot_lm <- function(d, model, conf.level = 0.95, reps = 1000){
model <- as.formula(model)

#Adding log values to formula 
 d <- d %>% mutate(
      logHR = log(HomeRange_km2),
      logBM = log(Body_mass_female_mean),
      logDL = log(DayLength_km))
  
#bootstrapping 
boot.strap <- d %>% 
  specify(model) %>% 
  generate(reps = reps, type = "bootstrap")
slope <- vector()
intercept <- vector()

z <- list()
  slope <- vector()
  intercept <- vector()
  x <- for (i in 1:reps) {
    z[[i]] <- sample_n(d, size = nrow(d), replace = TRUE)
    e <- lm(model, data = z[[i]])
    slope[[i]] <- e$coefficients[2]
    intercept[[i]] <- e$coefficients[1]
  }
  

boot.strap_lm <- tibble(
  slope = slope, 
  intercept = intercept)

#values 
alpha <- 1-conf.level
confidence_level <- conf.level
p_lower <- alpha / 2
p_upper <- 1 - (alpha / 2)
degrees_of_freedom <- nrow(boot.strap_lm) - 2
critical_value <- qt(p_upper, df = degrees_of_freedom)

#boot model coeffecients and values 
boot.strap_lm<-boot.strap_lm %>% 
  summarize(model="boot",
                beta1 = mean(slope), 
                beta1.std.error = sd(slope),
                beta1.ci.lower = quantile(slope, p_lower),
                beta1.ci.upper = quantile(slope, p_upper),
                beta0 = mean(intercept),
                beta0.std.error = sd(intercept),
                beta0.ci.lower = quantile(intercept, p_lower),
                beta0.ci.upper = quantile(intercept, p_upper),
)

#original model coeffecients and values 
originalmodel<-lm(model,data=d)

originaldataset<- tibble(model="original dataset linear model",
                         beta1 = originalmodel$coefficients[2],
                         beta1.std.error = tidy(originalmodel)$std.error[2],
                         beta1.ci.lower =  tidy(originalmodel, conf.int = TRUE, conf.level = conf.level)$conf.low[2],
                         beta1.ci.upper =  tidy(originalmodel, conf.int = TRUE, conf.level = conf.level)$conf.high[2],
                         beta0 = originalmodel$coefficients[1],
                         beta0.std.error = tidy(originalmodel)$std.error[1],
                         beta0.ci.lower = tidy(originalmodel, conf.int = TRUE, conf.level = conf.level)$conf.low[1],
                         beta0.ci.upper = tidy(originalmodel, conf.int = TRUE, conf.level = conf.level)$conf.high[1])

  
#combined output
output<-rbind(originaldataset,boot.strap_lm)

output

}
```

```{r challenge3 output, echo=FALSE}

boot_lm(d = d, model = "logHR ~ logBM")
boot_lm(d = d, model = "logDL ~ logBM")
boot_lm(d = d, model = "logHR ~ logBM + MeanGroupSize")

```

